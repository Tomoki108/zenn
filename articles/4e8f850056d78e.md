---
title: "[読書メモ] System Design Interview – An insider's guide"
emoji: "🧙🏼‍♂️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["読書メモ", "ソフトウェアアーキテクチャ"]
published: false
---

システムデザイン（インタビュー対策）に関するベストセラーの、個人的読書メモ。
[amazon リンク](https://www.amazon.co.jp/-/en/Alex-Xu/dp/B08CMF2CQF)

# Chapter 1: Scale From Zero To Millions Of Users

**全ての tier で冗長性と可用性を高めるべし。レイテンシーも低くすべし。**

- サーバーは 1 台固定ではなく最低 2 台以上を起動させ、**ロードバランサー**でリクエストを振り分ける。

  - 負荷に応じて台数を増やす。（**オートスケーリング**）
  - ロードバランサーの public ip のみ公開し、サーバー自体はロードバランサーから private ip によってアクセスする。

- DB は **primary/standby** 構成にし、前者に write リクエストを、後者に read リクエストを振り分け、primary=>secondary の**データレプリケーション**を行う。

  - primary に障害があれば standby を primary に昇格させ、standby を増やす。standby に障害があれば、standby を増やす。

- DB へ頻繁に問い合わせられ、かつ更新頻度の低いデータは、**キャッシュ**システムに保存する。

  - キャッシュには適切な有効期限を設定する。
  - 問い合わせに対してはキャッシュがあればそれを返し、なければ DB に問い合わせそれを返す。キャッシュにも保存する。
  - DB 更新とキャッシュの更新は、トランザクションを用い整合性が常に保たれるようにする。

- 静的データ（HTML、CSS、JS）は **CDN（Contents Delivery Network）** で提供する。

  - CDN の世界各地のサーバーの、ユーザーに最も近いものから自動で配信できる。

- サーバーは**ステートレス**にする。セッション（ユーザーの認証情報）をサーバー内の RAM やファイルシステムに保存しない。オートスケーリングやフェイルオーバーの障害になる。

  - セッション情報などは外部ストレージに保存する。速度が求められるので NoSQL も有力。

- システム群は、複数のデータセンターに冗長化する。DB 等はレプリケーションして一貫性を保つ。

  - ロードバランサーはユーザーの地理によって適切なデータセンターにルーティングする。（**GeoDNS-routed**）

- バッチ処理は、Worker サーバーを用意し、API サーバーから**メッセージキュー**を介してタスクに取り組む。API サーバーとは独立してスケーリングできるようになる。

- ツールを使いあらゆるログを収集し、ホストレベルメトリクス（ex, CPU usage）、集約レベルメトリクス（ex, DB tier performance）、KPI（ex, 収益、CR）などの**メトリクス**を監視する。見やすいダッシュボードを作る。

- DB を**シャーディング**によって水平スケーリングする。DB をシャードという単位に分割し、データを適切なパーティションキー（ex, id）から生成した ハッシュによって、どのシャードに格納するか切り分ける。高速なデータへのアクセスが可能になる。
  - データの増加に応じてシャードを追加したり、既存のシャードを更に分割するときは**リシャーディング**（データの再配置）を行う。
  - シャードにまたがるデータに対し連結クエリを実行したくなったときは、DB の非正規化でシャード内でデータが完結するようにしたり、**マテリアライズドビュー**を用意したりしておく。

# Chapter 2: Back-of-the-envelope Estimation

# Chapter 3: A Framework For System Design Interviews

## step1. 問題を認識して設計のスコープを明確にする / 3~10 min

「ニュースフィードを設計してください」

- web or mobile? both?
- 最低限のユースケースは何？
- Daily Active User の数は？
- 機能の詳細

  - フィードは友達の記事？時系列順？何らかの重み順？
  - ユーザーは友達を何人もてる？
  - ニュースはテキスト？イメージやビデオを含む？

## step2. 全体設計を提案し同意を得る / 10~15 min

::: details このくらいの全体設計をホワイトボードに書く
![image](/images/202502_system_design/chapter3_blueprint.png)
:::

## step3. 詳細を設計する / 10~25 min

どの部分を詳細に設計して欲しいのか明確にする。

重要なユースケース。ex, 「ニュースの投稿」「ニュースフィードの取得」
あるいは重要なコンポーネント。ex, 「フィードのキャッシュ」「リクエストのレートリミッター」

::: details 全体設計に書き足す
![image](/images/202502_system_design/chapter3_detailed_blueprint.png)
:::

## step4. Wrap Up / 3~5 min

これまでに作った設計をもう一度説明して振り返ると良い。

また、最後に以下の様なことが聞かれることが多い。

- ボトルネックと改善ポイント
- エラーケースからの復帰
- 運用の問題。どの様にログを取って監視する？新機能はどの様にリリース（ロールアウト）する？
- ユーザーが 10 倍になったらどうする？

## Dos

- 確認のために常に質問を。勝手に前提を置かない。
- 要件を理解する。不確かなまま進めない。
- 可能なら複数の提案を。アイデアをインタビュアーにぶつけよう。（インタビュアーは仲間。）
- **全体設計に合意してから**、重要な場所の詳細設計に入る。

## Dont's

- **典型的な質問への対策は怠らない。**
- 要件や期待を確認するまで、ソリューションにジャンプしない。
- 質問を臆さない、黙って考えない。
- インタビュアーが終わりというまで終わらない。

# Chapter 4: Design A Rate Limiter

## step1. 問題を認識して設計のスコープを明確にする / 3~10 min

- server side or client side?
  - => sever side
- 何でレート制限する？IP、UserID？
  - => up to you
- システムのスケールは？スタートアップ？巨大なシステム？
  - => 大規模なユーザーにも対応できる様にして。
- 分散型システムでも機能する必要はある？
  - => yes
- ユーザーにレート制限中と伝える必要はある？
  - => yes

## step2. 全体設計を提案し同意を得る / 10~15 min

### どこにレートリミッターを置くか

- サーバー
  - 実装が必要だがリミッターの挙動を比較的自由にできる。分散システムと相性が悪い。
- ミドルウェア（外部サービス）
  - 楽。利用するサービスによってはリミッターの挙動が限られたオプションになる。分散システムと相性が良い。

### どこにレートリミット用のカウンター（またはログなど）を保存するか

- DB は適さない。ディスクアクセスは遅い。
- In-Memory Cache （Redis など）が適当。データを一定期間で expired にもできる。

::: details blueprint
![image](/images/202502_system_design/chapter4_blueprint.png)
:::

## step3. 詳細を設計する / 10~25 min

### レートリミットのルールはどこに保存されている？

- 何らかの永続化ディスクに保存する。Worker が定期的にディスクにアクセスしてキャッシュを作成する。ミドルウェアはキャッシュを参照する。

### リミットオーバーのリクエストのハンドリングはどうなる？

- ミドルウェアが Redis にカウンター等の情報を取得。キャッシュから取得したルールと照合して、
  - リミットオーバーなら 429。header に`X-Ratelimit-Retry-After`などを付与。リクエスト自体はドロップするか、メッセージキューに入れる。（要件次第。）
  - オーバーしていないならサーバーへ処理を進める。Redis へカウンター等も記録する。レスポンスヘッダーに`X-Ratelimit-Remaining`, `X-Ratelimit-Limit`などを付与する。

::: details detailed blueprint
![imgae](/images/202502_system_design/chapter4_detailed_blueprint.png)
:::

### 分散システムだったとして、気にする必要があることは？

- 状態の競合（Race Condition）。Dirty Read など。transaction lock するのがシンプルだが遅くなる。「Lua Script and sorted sets data structure in Redis」というソリューションが有名だったりする。

- 同期の問題。Redis 等を centerized Data store として使うと良い。各レートリミッターが別々のデータストアを参照している状態は、スケーラブルじゃない。

###　モニタリング

- レートリミットアルゴリズムが適切かどうか見た方がいい。

  - 例えばタイムセールなどのトラフィックバーストに対応できていない様だったら、Token Bucket への変更を考えるなど。

- レートリミットのルールが適切かどうか見た方がいい。
  - 通常の範囲のリクエストが跳ねられていたら問題。

## step4. Wrap Up / 3~5 min

以下の様な話題を出してもいい。

- Hard vs soft rate limiting?
  - リミット超過のリクエストは認めないのか、短時間だけ認めるか。
- アプリケーションレイヤー以外でのレートリミット。
  - ex, Iptable (OSI layer 3)での IP 制限。
- クライアント側の効率化
  - クライアントキャッシュを利用して API call を減らす
  - レートリミットをコードに織り込む。429 をハンドリングする。

---

## レートリミットの各種アルゴリズム

::: details Token Bucket / Leaking Bucket / Fixed Window Counter / Sliding Window Log / Sliding Window Counter

## Token Bucket

- バケットに決められた時間に決められた量のトークンを補充。キャパオーバーなら補充しない。（ex, cap 4 のバケットに 2 tokens/sec を補充。）
- ユーザーリクエストのたびにトークンを一つ消費。トークンが足りなければ 429。
- ユーザーごと \* API Endpoint ごとにバケットを用意する。

- pros
  - token が残っている限りリクエストできるため、短いトラフィックのバーストに対応可能。
  - `token_count, last_refill_time, cap, rate`を保持するだけでいいため、メモリ効率がいい。
- cons
  - バケットのキャパ、トークンのリフィルレートのチューニングが難しい。

## Leaking Bucket

- サイズが設定してある FIFO キューにリクエストをため、等間隔（ex, 1 req/sec）で処理していく。キューがいっぱいの場合 429。

- pros
  - キューのサイズ上限によりメモリ効率がいい。
  - 等間隔でのリクエスト処理が必要な場合に適している。
- cons
  - 古いリクエストでキューがいっぱいの時に、それ以降のリクエストが全て処理されない。
  - キューのサイズ、リクエストの処理のレートのチューニングが難しい。

## Fixed Window Counter

- 時刻をウィンドウに分割。（ex, 1:00 00~59, 1:01 00 ~59, ...）ウィンドウごとにリクエスト上限までしかリクエストできない。リクエストごとにウィンドウのカウンターをプラスし、上限を超えていれば 429。

- pros
  - `window_start, window_size, conter`だけ保持すればいいのでメモリ効率がいい。
  - 一定時間でクォータをリセットできる。
- cons
  - トラフィックバーストの際に、ウィンドウ i の終わり、ウィンドウ i+1 の始まりを見ると 100 req / 1 min のような決まったレートリミットを超えていることがある。

### Sliding Window Log

- リクエストのタイムスタンプをログに記録する。2 req/min だった場合、リクエストが来た時に直近一分のログが 1 個以下ならリクエストを通す。そうでないならログを記録しつつ 429。直近一分より前のログは削除する。

- pros
  - 正確なレート制限。上記例だと、どの一分を切り取ってもリクエストは 2 件以上処理されない。
- cons
  - メモリ効率が悪い。リクエストがリジェクトされてもログに残る。バーストの時に大量のログが残る。

### Sliding Window Counter

- Fixed Window Counter と同様に時刻をウィンドウに分割し、ウィンドウごとに上限までしかリクエストできない。カウンターで管理。
- ただし、あるウィンドウで開始時刻からウィンドウサイズの例えば 30%の時刻に到達するまでは、リクエスト上限が「前のウィンドウのカウンターの合計 \* 70%」を引いたものになる。

- pros
  - Fixed Window Counter と同様にメモリ効率がいい。
  - Fixed Window Counter の、ウィンドウの境目前後で見るとリクエスト上限を超えかねない問題を概ね克服している。
- cons
  - ウィンドウの境目でリクエスト上限を超えないことを完全には保証していない。ただし理論上ごくごく低確率らしい。

:::

# Chapter 5: Design Consistent Hashing

# Chapter 6: Design A Key-value Store

# Chapter 7: Design A Unique Id Generator In Distributed Systems

# Chapter 8: Design A Url Shortener

# Chapter 9: Design A Web Crawler

# Chapter 10: Design A Notification System

# Chapter 11: Design A News Feed System

# Chapter 12: Design A Chat System

# Chapter 13: Design A Search Autocomplete System

# Chapter 14: Design Youtube

# Chapter 15: Design Google Drive

# Chapter 16: The Learning Continues
