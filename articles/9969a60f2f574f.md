---
title: "10分間で、200万回APIリクエストをし結果を元に200万件のデータを更新する"
emoji: "🦔"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [go]
published: false
---

こんにちは、バックエンドエンジニアの永田です。

本日は表題のような大量の API リクエストを伴うバッチ処理を実装したお話をしようと思います。

## バッチの概要

CastiongONE は主に派遣会社（以降テナントと呼称します）に使っていただいている SaaS なのですが、テナントが LINE 公式アカウントを運用している場合があります。

今回のバッチはテナントが抱えている各求職者について、「テナントの LINE 公式アカウントと友達であるかどうか（= LINE 友達ステータス）」の情報を毎日更新するためのものになります。

求職者が CastingONE の求職者向けサイト と LINE ログイン連携（OAuth 連携）をしている場合、その LINE ID が DB に保存されています。それを元に [LINE API](https://developers.line.biz/ja/reference/messaging-api/#get-profile) を一人づつ叩いて確認し、結果を元に LINE 友達ステータスを DB に反映しています。(複数人の友達ステータスをまとめて確認できる API は現在のところ存在しないようでした。)

<i>※ 実は LINE Developer Console 上では Webhook URL を登録することができ、アカウントに対して友達追加やブロックといったイベントが発生したときに外部システムに通知を飛ばすこともできます。しかし今回は諸事情により、 API を毎日定期バッチから叩くことでステータス更新を行う方式にしました。（諸事情については割愛します。）</i>

### 非機能要件

「LINE 公式アカウントを運用しているテナントに紐づく求職者」の数は、合計で大体 200 万人ほどでした。
これらの求職者の LINE 友達ステータス更新するにあたって以下の二つの非機能要件があり、これらに気を使い実装する必要がありました。

- バッチ実行は数十分以内に終わらせる
- [LINE API のリクエストレートリミット（チャネルごとに 2000 リクエスト/秒）](https://developers.line.biz/ja/reference/messaging-api/#get-profile-rate-limit)を超えないようにする

## コード

いきなりですが、実際のコードは以下のような形になりました。DDD における UseCase の実装部分について、簡素化したものを載せています。

<i>※ デバグのためのログ出力やエッジケースのハンドリング、private 関数のコードなどが省略されています。</i>

```usecase.go
package application

import (
    "context"
    "sync"
    "time"

    "github.com/CastingONE/castingone/go/jobs/batch-update-job-seeker-line-friend-status/domain"
    "github.com/CastingONE/castingone/go/pkg/logger"
    "golang.org/x/sync/semaphore"
    "golang.org/x/time/rate"
)

// LINE友達ステータス取得の並列実行数
const getConcurrencyLimit = 200

// LINE友達ステータス取得のリクエストレートリミットのための間隔
const getInterval = 1 / 2000 * time.Second

type UseCase struct {
    JobSeekerService           domain.JobSeekerService
    LineOfficialAccountService domain.LineOfficialAccountService
    LineAPI                    domain.LineAPI
}

func (uc *UseCase) BatchUpdateJobSeekerLineFriendStatus(ctx context.Context) (err error) {
    // テナントごとのLINEチャンネルアクセストークンを取得する（APIリクエストに必要）
    tenantIDChannelAccessTokenMap, err := uc.LineOfficialAccountService.GetTenantIDLineChannelAccessTokenMap(ctx)
    if err != nil {
        return err
    }

    // テナントごとに、CastingONEが現在保持している求職者のLINE友達ステータスを取得する
    jobSeekerLineFriendStatusList, err := uc.JobSeekerService.GetJobSeekerLineFriendStatusList(ctx)
    if err != nil {
        return err
    }

    // テナントに紐づく求職者ごとに処理を行う
    for tenantID, jobSeekerLineFriendStatusList := range jobSeekerLineFriendStatusList.ToTenantIDMap() {
        cctx := context.WithoutCancel(ctx)
        channelAccessToken, ok := tenantIDChannelAccessTokenMap[tenantID]
        if !ok {
            logger.WarnfWithContext(cctx, "channelAccessToken not found for tenantID: %d", tenantID)
            continue
        }

        var wg sync.WaitGroup
        getSem := semaphore.NewWeighted(int64(getConcurrencyLimit))
        getBuff := rate.NewLimiter(rate.Every(getInterval), 1)
        getResults := make(chan *domain.JobSeekerLineFriendStatus, len(jobSeekerLineFriendStatusList))

        // 求職者の現在のLINE友達ステータスを、並列でLINE APIから取得する
        for _, jobSeekerLineFriendStatus := range jobSeekerLineFriendStatusList {
            if err := getSem.Acquire(cctx, 1); err != nil {
                logger.ErrorWithContext(cctx, err)
                continue
            }
            if err := getBuff.Wait(cctx); err != nil {
                getSem.Release(1)
                logger.ErrorWithContext(cctx, err)
                continue
            }
            wg.Add(1)

            go func() {
                defer getSem.Release(1)
                defer wg.Done()
                defer func() {
                    if r := recover(); r != nil {
                        logger.ErrorfWithContext(cctx, "panic occurred for tenantID: %d, jobSeekerID: %d", tenantID, jobSeekerLineFriendStatus.ID)
                    }
                }()

                // 求職者のLINE友達ステータスを取得し、結果をチャネルに送信する
                uc.getLineFriendStatus(cctx, channelAccessToken, jobSeekerLineFriendStatus, getResults)
            }()
        }

        // 全ての求職者のLINE友達ステータス取得が完了するまで待機し、結果を更新用のスライスに入れる
        wg.Wait()
        close(getResults)

        updateSlice := make(domain.JobSeekerLineFriendStatusList, 0, len(getResults))
        for jobSeekerLineFriendStatus := range getResults {
            updateSlice = append(updateSlice, jobSeekerLineFriendStatus)
        }

        // 求職者のLINE友達ステータスを一括更新する
        if err := uc.JobSeekerService.BatchUpdateJobSeekerLineFriendStatus(cctx, tenantID, updateSlice); err != nil {
            logger.ErrorWithContext(cctx, err)
        }
    }

    return nil
}

```

### ポイント

200 万回もの API リクエストを数十分以内に完了しないといけないため、どこかで並列処理を行う必要がありました。以下の３つの方針が考えられると思います。

1. 各テナントごとの処理を並列化する
2. 各 API リクエストに関する処理を並列化する
3. 1 と 2 の両方を並列化する

方針 3 については、Goroutine をネストさせると可読性や保守性が下がるため、最初に除外しました。そして当初は、方針 1 による実装を考えていました。API へのリクエストレートリミットによって各リクエスト間でバッファを持たなければならないため、そこを並列化をしても処理速度が早くならないと思ったからでです。

しかし最終的には方針 2 を選択しました。実際の API のレスポンス速度を計測したところ、平均 0.06 秒でした。レートリミットを守るためのバッファは 1 / 2000 = 0.0005 秒であるため、大量の API リクエストを直列で行った場合、速度のボトルネックになるのはレートリミットではなくレスポンス待ちの時間だったわけです。つまり、並列化による高速化の恩恵は多分にあることになります。

テナントごとに並列化を行い、その中での API リクエストは直列で行う場合に比べて、時間のかかるレスポンス待ちをより多く並列でこなすことができるので方針 2 の方が大幅に処理時間の短縮ができます。（テナントの総数は 200 前後で、各テナントに紐づく求職者は数千~数十万のためそうなります。）結果的に、毎日の定期実行は 10 分程度で完了しています。

Goroutine の最大並列数は、レスポンス速度とバッファの時間をもとに設定しました。バッファ分スリープしつつ Goroutine を作成していった場合 120 個まで Gouroutine の数が増えていきます。121 回目のリクエストを投げる前に 1 回目のリクエストへのレスポンスが返ってくるため、それ以上 Goroutine の数は増えないはずです。このことから、最大並列数は余裕を持って 200 個に設定しました。

## 学び

### Gorutine はとっても軽量！

当初各テナントごとの処理の方を並列化しようと考えた理由として、自分が Gouroutine についてせいぜい数十個程度しか作成できないという勘違いをしていたこともありました。既存のコードでは、その程度の数の Goroutine を作成しているものしかなかったからです。

しかし調べてみると Goroutine 自体が使うメモリは数 KB 程度だったので、並列数は Goroutine 内の処理で使う CPU やメモリの大きさに規定されることになります。Gorotinen 内の処理が軽量であれば、（サーバーのスペックが許す範囲で）並列数は数百でも数千でも増やすことができます。

### 大量の API リクエストの処理時間は、レートリミットのためのバッファではなくレスポンス速度に規定される

（もちろんレートリミットとレスポンス速度の値によりますが、往々にして後者の方が遅いのではないかと思います。）

前段のポイントに記載していたことですが自分は最初これに気づいていなかったので、指摘していただいた同じチームの hiro さんに感謝です。

## 感想

今回のバッチ実装は Goroutine や Channel、WaitGroup、Semaphore、Buffer などをふんだんに活用したものになっています。個人的に前職ではずっと PHP を書いており、こういったものを実装してみたくて CastingONE に転職した背景があます。作っていて非常に楽しく、PHP じゃ絶対書けないな、Go すごい...ということで興奮しておりました。

またこの記事では触れていませんが、[spf13/cobra](https://github.com/spf13/cobra)を使ったコマンドの実装や、Skaffold を使った Cloud Run Jobs へのバッチのデプロイなど、自分にとっては経験のない技術を扱うことができたのも僥倖でした。
